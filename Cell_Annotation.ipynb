{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23823,"databundleVersionId":1920183,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":4885048,"sourceType":"datasetVersion","datasetId":2482568},{"sourceId":10946059,"sourceType":"datasetVersion","datasetId":6808267},{"sourceId":278050,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238150,"modelId":259817},{"sourceId":278068,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238166,"modelId":259833}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning on Pre-trained Model for Cell-type Annotation\nIn this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the cell type annotation task. We use the Multiple Sclerosis dataset as an example and fine-tune on the pre-trained whole-body model. Please download the dataset folder from https://drive.google.com/drive/folders/1Qd42YNabzyr2pWt9xoY4cVMTAxsNBt4v?usp=sharing\n\nWe summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on cell-type annotation tasks and beyond: \n\n     1. Specify hyper-parameter setup for integration task\n     \n     2. Load and pre-process data\n     \n     3. Load the pre-trained scGPT model\n     \n     4. Finetune scGPT with task-specific objectives\n     \n     5. Evaluate fine-tuned scGPT","metadata":{}},{"cell_type":"code","source":"!pip install scgpt scanpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport logging\n\n# # Set up a logger for this module\n# logger = logging.getLogger(__name__)\n\n#main code logger different - yash\n# Create a dedicated logger for API usage tracking\napi_usage_logger = logging.getLogger(\"torch_api_usage_logger\")\napi_usage_logger.setLevel(logging.INFO)  # Set the desired log level\n\n# Optionally, add a handler if you want to output to a file or change format\nhandler = logging.StreamHandler()  # or logging.FileHandler('api_usage.log')\nformatter = logging.Formatter('%(asctime)s - %(message)s')\nhandler.setFormatter(formatter)\napi_usage_logger.addHandler(handler)\n\ndef _log_class_usage(klass):\n    \"\"\"Log class usage for API tracking.\n    \n    Args:\n        klass: The class whose usage is being logged.\n    \"\"\"\n    identifier = \"custom_class\"\n    if klass and hasattr(klass, \"__name__\"):\n        identifier += f\".{klass.__name__}\"\n\n    # Log the usage of the class with Torch's internal logging mechanism\n    torch._C._log_api_usage_once(identifier)\n\n    # Optionally log the usage using standard Python logging as well\n    logger.info(f\"API usage logged for class: {identifier}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:44.967600Z","iopub.execute_input":"2025-03-08T09:42:44.967997Z","iopub.status.idle":"2025-03-08T09:42:47.107323Z","shell.execute_reply.started":"2025-03-08T09:42:44.967954Z","shell.execute_reply":"2025-03-08T09:42:47.106102Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from typing import Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\n\n\nclass Vocab(nn.Module):\n    __jit_unused_properties__ = [\"is_jitable\"]\n    r\"\"\"Creates a vocab object which maps tokens to indices.\n\n    Args:\n        vocab (torch.classes.torchtext.Vocab or torchtext._torchtext.Vocab): a cpp vocab object.\n    \"\"\"\n    # Removed 'torchtext' specific reference in docstring\n    r\"\"\"Creates a vocab object which maps tokens to indices.\n\n    Args:\n        vocab (Vocab): A Python-based vocab object that implements necessary methods like `lookup_indices` etc.\n    \"\"\"\n\n    def __init__(self, vocab) -> None:\n        super(Vocab, self).__init__()\n        self.vocab = vocab\n        _log_class_usage(__class__)\n\n\n    @property\n    def is_jitable(self):\n        return isinstance(self.vocab, torch._C.ScriptObject)\n\n    @torch.jit.export\n    def forward(self, tokens: List[str]) -> List[int]:\n        r\"\"\"Calls the `lookup_indices` method\n\n        Args:\n            tokens: a list of tokens used to lookup their corresponding `indices`.\n\n        Returns:\n            The indices associated with a list of `tokens`.\n        \"\"\"\n        return self.vocab.lookup_indices(tokens)\n\n\n    @torch.jit.export\n    def __len__(self) -> int:\n        r\"\"\"\n        Returns:\n            The length of the vocab.\n        \"\"\"\n        return len(self.vocab)\n\n\n    @torch.jit.export\n    def __contains__(self, token: str) -> bool:\n        r\"\"\"\n        Args:\n            token: The token for which to check the membership.\n\n        Returns:\n            Whether the token is member of vocab or not.\n        \"\"\"\n        return self.vocab.__contains__(token)\n\n\n    def __getitem__(self, token: str) -> int:\n        r\"\"\"\n        Args:\n            token: The token used to lookup the corresponding index.\n\n        Returns:\n            The index corresponding to the associated token.\n        \"\"\"\n        return self.vocab[token]\n\n\n    @torch.jit.export\n    def set_default_index(self, index: Optional[int]) -> None:\n        r\"\"\"\n        Args:\n            index: Value of default index. This index will be returned when OOV token is queried.\n        \"\"\"\n        self.vocab.set_default_index(index)\n\n\n    @torch.jit.export\n    def get_default_index(self) -> Optional[int]:\n        r\"\"\"\n        Returns:\n            Value of default index if it is set.\n        \"\"\"\n        return self.vocab.get_default_index()\n\n\n    @torch.jit.export\n    def insert_token(self, token: str, index: int) -> None:\n        r\"\"\"\n        Args:\n            token: The token used to lookup the corresponding index.\n            index: The index corresponding to the associated token.\n        Raises:\n            RuntimeError: If `index` is not in range [0, Vocab.size()] or if `token` already exists in the vocab.\n        \"\"\"\n        self.vocab.insert_token(token, index)\n\n\n    @torch.jit.export\n    def append_token(self, token: str) -> None:\n        r\"\"\"\n        Args:\n            token: The token used to lookup the corresponding index.\n\n        Raises:\n            RuntimeError: If `token` already exists in the vocab\n        \"\"\"\n        self.vocab.append_token(token)\n\n\n    @torch.jit.export\n    def lookup_token(self, index: int) -> str:\n        r\"\"\"\n        Args:\n            index: The index corresponding to the associated token.\n\n        Returns:\n            token: The token used to lookup the corresponding index.\n\n        Raises:\n            RuntimeError: If `index` not in range [0, itos.size()).\n        \"\"\"\n        return self.vocab.lookup_token(index)\n\n\n    @torch.jit.export\n    def lookup_tokens(self, indices: List[int]) -> List[str]:\n        r\"\"\"\n        Args:\n            indices: The `indices` used to lookup their corresponding`tokens`.\n\n        Returns:\n            The `tokens` associated with `indices`.\n\n        Raises:\n            RuntimeError: If an index within `indices` is not int range [0, itos.size()).\n        \"\"\"\n        return self.vocab.lookup_tokens(indices)\n\n\n    @torch.jit.export\n    def lookup_indices(self, tokens: List[str]) -> List[int]:\n        r\"\"\"\n        Args:\n            tokens: the tokens used to lookup their corresponding `indices`.\n\n        Returns:\n            The 'indices` associated with `tokens`.\n        \"\"\"\n        return self.vocab.lookup_indices(tokens)\n\n\n    @torch.jit.export\n    def get_stoi(self) -> Dict[str, int]:\n        r\"\"\"\n        Returns:\n            Dictionary mapping tokens to indices.\n        \"\"\"\n        return self.vocab.get_stoi()\n\n\n    @torch.jit.export\n    def get_itos(self) -> List[str]:\n        r\"\"\"\n        Returns:\n            List mapping indices to tokens.\n        \"\"\"\n        return self.vocab.get_itos()\n\n\n    # def __prepare_scriptable__(self):\n    #     r\"\"\"Return a JITable Vocab.\"\"\"\n    #     if not self.is_jitable:\n    #         cpp_vocab = torch.classes.torchtext.Vocab(self.vocab.itos_, self.vocab.default_index_)\n    #         return Vocab(cpp_vocab)\n    #     return self\n\n    def __prepare_scriptable__(self):\n        r\"\"\"Return a JITable Vocab.\"\"\"\n        if not self.is_jitable:\n            # Assuming the `vocab` is a Python-based vocab object with necessary methods\n            cpp_vocab = Vocab(self.vocab.itos_, self.vocab.default_index_)\n            return Vocab(cpp_vocab)\n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:47.108525Z","iopub.execute_input":"2025-03-08T09:42:47.109437Z","iopub.status.idle":"2025-03-08T09:42:47.416101Z","shell.execute_reply.started":"2025-03-08T09:42:47.109405Z","shell.execute_reply":"2025-03-08T09:42:47.415088Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"VocabPybind is not required if you're using your Python-based Vocab implementation.\nVocabPybind would work correctly only if you are utilizing torchtext's C++ Vocab, which would involve Pybind11 integration, not applicable to your current code.","metadata":{}},{"cell_type":"code","source":"# %%\nimport copy\nimport gc\nimport json\nimport os\nfrom pathlib import Path\nimport shutil\nimport sys\nimport time\nimport traceback\nfrom typing import List, Tuple, Dict, Union, Optional\nimport warnings\nimport pandas as pd\n# from . import asyn\nimport pickle\nimport torch\nfrom anndata import AnnData\nimport scanpy as sc\n# import scvi\nimport seaborn as sns\nimport numpy as np\nimport wandb\nfrom scipy.sparse import issparse\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n# from torchtext.vocab import Vocab\n# from torchtext._torchtext import (\n#     Vocab as VocabPybind,\n# )\nfrom sklearn.metrics import confusion_matrix\n\nsys.path.insert(0, \"../\")\nimport scgpt as scg\nfrom scgpt.model import TransformerModel, AdversarialDiscriminator\nfrom scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\nfrom scgpt.loss import (\n    masked_mse_loss,\n    masked_relative_error,\n    criterion_neg_log_bernoulli,\n)\nfrom scgpt.tokenizer.gene_tokenizer import GeneVocab\nfrom scgpt.preprocess import Preprocessor\nfrom scgpt import SubsetsBatchSampler\nfrom scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n\nsc.set_figure_params(figsize=(6, 6))\nos.environ[\"KMP_WARNINGS\"] = \"off\"\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:47.417095Z","iopub.execute_input":"2025-03-08T09:42:47.417372Z","iopub.status.idle":"2025-03-08T09:42:56.042411Z","shell.execute_reply.started":"2025-03-08T09:42:47.417337Z","shell.execute_reply":"2025-03-08T09:42:56.041356Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n  warnings.warn(\"flash_attn is not installed\")\n/usr/local/lib/python3.10/dist-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n  warnings.warn(\"flash_attn is not installed\")\n/usr/local/lib/python3.10/dist-packages/scanpy/_settings.py:490: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  IPython.display.set_matplotlib_formats(*ipython_format)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step1: Specify hyper-parameter setup for cell-type annotation task\nListed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification.","metadata":{}},{"cell_type":"code","source":"hyperparameter_defaults = dict(\n    seed=0,\n    dataset_name=\"cite_seq\",\n    do_train=True,\n    load_model=\"/kaggle/input/scgpt_human/keras/default/1/scGPT_human\",\n    mask_ratio=0.0,\n    epochs=10, #10\n    n_bins=51,\n    MVC=False, # Masked value prediction for cell embedding\n    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n    dab_weight=0.0,\n    lr=1e-4, #1e-4\n    batch_size=32, #32\n    layer_size=128,\n    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder #4\n    nhead=4,  # number of heads in nn.MultiheadAttention #4\n    dropout=0.2,  # dropout probability\n    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n    save_eval_interval=1, #5\n    fast_transformer=True,\n    pre_norm=False,\n    amp=True,  # Automatic Mixed Precision\n    include_zero_gene = False,\n    freeze = False, #freeze\n    DSBN = False,  # Domain-spec batchnorm\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:56.048707Z","iopub.execute_input":"2025-03-08T09:42:56.048976Z","iopub.status.idle":"2025-03-08T09:42:56.053989Z","shell.execute_reply.started":"2025-03-08T09:42:56.048951Z","shell.execute_reply":"2025-03-08T09:42:56.053315Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"wandb.finish()\n!wandb login cc1eea9dcc7b7e94199b8a914fdd5edce065d074","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:56.054497Z","iopub.execute_input":"2025-03-08T09:42:56.054755Z","iopub.status.idle":"2025-03-08T09:42:58.434672Z","shell.execute_reply.started":"2025-03-08T09:42:56.054731Z","shell.execute_reply":"2025-03-08T09:42:58.433805Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"run = wandb.init(\n    config=hyperparameter_defaults,\n    project=\"scGPT\",\n    reinit=True,\n    resume = False, #yash\n    settings=wandb.Settings(start_method=\"fork\"),\n)\nconfig = wandb.config\nprint(config)\n\n# Assuming `set_seed` is defined elsewhere\nset_seed(config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:42:58.435863Z","iopub.execute_input":"2025-03-08T09:42:58.436207Z","iopub.status.idle":"2025-03-08T09:43:12.351918Z","shell.execute_reply.started":"2025-03-08T09:42:58.436171Z","shell.execute_reply":"2025-03-08T09:43:12.351177Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myashshri148\u001b[0m (\u001b[33myashshri148-indian-institute-of-technology-mandi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250308_094305-0f4xfcwt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT/runs/0f4xfcwt' target=\"_blank\">visionary-grass-24</a></strong> to <a href='https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT' target=\"_blank\">https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT/runs/0f4xfcwt' target=\"_blank\">https://wandb.ai/yashshri148-indian-institute-of-technology-mandi/scGPT/runs/0f4xfcwt</a>"},"metadata":{}},{"name":"stdout","text":"{'seed': 0, 'dataset_name': 'cite_seq', 'do_train': True, 'load_model': '/kaggle/input/scgpt_human/keras/default/1/scGPT_human', 'mask_ratio': 0.0, 'epochs': 2, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0002, 'batch_size': 8, 'layer_size': 128, 'nlayers': 3, 'nhead': 3, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 1, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# settings for input and preprocessing\npad_token = \"<pad>\"\nspecial_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\nmask_ratio = config.mask_ratio\nmask_value = \"auto\"  # for masked values, now it should always be auto\n\ninclude_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\nmax_seq_len = 3001\nn_bins = config.n_bins\n\n# input/output representation\ninput_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\noutput_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n\n# settings for training\nMLM = False  # whether to use masked language modeling, currently it is always on.\nCLS = True  # celltype classification objective\nADV = False  # Adversarial training for batch correction\nCCE = False  # Contrastive cell embedding objective\nMVC = config.MVC  # Masked value prediction for cell embedding\nECS = config.ecs_thres > 0  # Elastic cell similarity objective\nDAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\nINPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\ninput_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\ncell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\nadv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\nadv_D_delay_epochs = 0\nmvc_decoder_style = \"inner product\"\necs_threshold = config.ecs_thres\ndab_weight = config.dab_weight\n\nexplicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\ndo_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n\nper_seq_batch_sample = False\n\n# settings for optimizer\nlr = config.lr  # TODO: test learning rate ratio between two tasks\nlr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\nbatch_size = config.batch_size\neval_batch_size = config.batch_size\nepochs = config.epochs\nschedule_interval = 1\n\n# settings for the model\nfast_transformer = config.fast_transformer\nfast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\nembsize = config.layer_size  # embedding dimension\nd_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\nnlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\nnhead = config.nhead  # number of heads in nn.MultiheadAttention\ndropout = config.dropout  # dropout probability\n\n# logging\nlog_interval = 100  # iterations\nsave_eval_interval = config.save_eval_interval  # epochs\ndo_eval_scib_metrics = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:12.352731Z","iopub.execute_input":"2025-03-08T09:43:12.352967Z","iopub.status.idle":"2025-03-08T09:43:12.359695Z","shell.execute_reply.started":"2025-03-08T09:43:12.352934Z","shell.execute_reply":"2025-03-08T09:43:12.359026Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# %% validate settings\nassert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\nassert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\nassert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\nif input_style == \"binned\":\n    if input_emb_style == \"scaling\":\n        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\nelif input_style == \"log1p\" or input_style == \"normed_raw\":\n    if input_emb_style == \"category\":\n        raise ValueError(\n            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n        )\n\nif input_emb_style == \"category\":\n    mask_value = n_bins + 1\n    pad_value = n_bins  # for padding gene expr values\n    n_input_bins = n_bins + 2\nelse:\n    mask_value = -1\n    pad_value = -2\n    n_input_bins = n_bins\n\nif ADV and DAB:\n    raise ValueError(\"ADV and DAB cannot be both True.\")\nDAB_separate_optim = True if DAB > 1 else False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:12.360685Z","iopub.execute_input":"2025-03-08T09:43:12.360985Z","iopub.status.idle":"2025-03-08T09:43:12.376385Z","shell.execute_reply.started":"2025-03-08T09:43:12.360957Z","shell.execute_reply":"2025-03-08T09:43:12.375626Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset_name = config.dataset_name\nsave_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}_logfile/\")\nsave_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"save to {save_dir}\")\nlogger = scg.logger\nscg.utils.add_file_handler(logger, save_dir / \"run.log\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:12.377211Z","iopub.execute_input":"2025-03-08T09:43:12.377494Z","iopub.status.idle":"2025-03-08T09:43:12.392810Z","shell.execute_reply.started":"2025-03-08T09:43:12.377465Z","shell.execute_reply":"2025-03-08T09:43:12.392142Z"}},"outputs":[{"name":"stdout","text":"save to save/dev_cite_seq-Mar08-09-43_logfile\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Step 2: Load and pre-process data\nWe follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prpocessing steps need to be applied to both of them.","metadata":{}},{"cell_type":"code","source":"import scanpy as sc\n\n# Load the dataset\nadata = sc.read(\"/kaggle/input/citeseqscrnaseqproteins-challenge-neurips2021/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad\")\n\n# Print basic summary of the dataset\nprint(adata)\n\n# Check the available columns in the observations (cell-level metadata)\nprint(adata.obs.columns)\n\n# Check the available columns in the variables (gene or protein-level metadata)\nprint(adata.var.columns)\n\n# If you want to inspect some of the data\nprint(adata.obs.head())  # Preview the cell-level metadata\nprint(adata.var.head())  # Preview the gene/protein-level metadata\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:12.393746Z","iopub.execute_input":"2025-03-08T09:43:12.394020Z","iopub.status.idle":"2025-03-08T09:43:25.246940Z","shell.execute_reply.started":"2025-03-08T09:43:12.393993Z","shell.execute_reply":"2025-03-08T09:43:25.246027Z"}},"outputs":[{"name":"stdout","text":"AnnData object with n_obs × n_vars = 90261 × 14087\n    obs: 'GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors', 'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts', 'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'is_train'\n    var: 'feature_types', 'gene_id'\n    uns: 'dataset_id', 'genome', 'organism'\n    obsm: 'ADT_X_pca', 'ADT_X_umap', 'ADT_isotype_controls', 'GEX_X_pca', 'GEX_X_umap'\n    layers: 'counts'\nIndex(['GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors',\n       'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts',\n       'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order',\n       'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality',\n       'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType',\n       'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker',\n       'is_train'],\n      dtype='object')\nIndex(['feature_types', 'gene_id'], dtype='object')\n                         GEX_n_genes_by_counts  GEX_pct_counts_mt  \\\nGCATTAGCATAAGCGG-1-s1d1                    893           6.723979   \nTACAGGTGTTAGAGTA-1-s1d1                   2606           8.008829   \nAGGATCTAGGTCTACT-1-s1d1                   1867           6.959707   \nGTAGAAAGTGACACAG-1-s1d1                   2360           6.109234   \nTCCGAAAAGGATCATA-1-s1d1                    455           0.294394   \n\n                         GEX_size_factors GEX_phase  \\\nGCATTAGCATAAGCGG-1-s1d1          0.356535        G1   \nTACAGGTGTTAGAGTA-1-s1d1          1.292643         S   \nAGGATCTAGGTCTACT-1-s1d1          0.970558       G2M   \nGTAGAAAGTGACACAG-1-s1d1          1.232604       G2M   \nTCCGAAAAGGATCATA-1-s1d1          0.044585         S   \n\n                         ADT_n_antibodies_by_counts  ADT_total_counts  \\\nGCATTAGCATAAGCGG-1-s1d1                         115            2828.0   \nTACAGGTGTTAGAGTA-1-s1d1                         137            8819.0   \nAGGATCTAGGTCTACT-1-s1d1                         116            4088.0   \nGTAGAAAGTGACACAG-1-s1d1                         124            4447.0   \nTCCGAAAAGGATCATA-1-s1d1                         132           12875.0   \n\n                         ADT_iso_count            cell_type batch  \\\nGCATTAGCATAAGCGG-1-s1d1            5.0  Naive CD20+ B IGKC+  s1d1   \nTACAGGTGTTAGAGTA-1-s1d1           21.0           CD14+ Mono  s1d1   \nAGGATCTAGGTCTACT-1-s1d1           12.0  Naive CD20+ B IGKC+  s1d1   \nGTAGAAAGTGACACAG-1-s1d1            9.0                  HSC  s1d1   \nTCCGAAAAGGATCATA-1-s1d1           24.0         Reticulocyte  s1d1   \n\n                         ADT_pseudotime_order  ...  DonorID DonorAge DonorBMI  \\\nGCATTAGCATAAGCGG-1-s1d1                   NaN  ...    15078       34     24.8   \nTACAGGTGTTAGAGTA-1-s1d1                   NaN  ...    15078       34     24.8   \nAGGATCTAGGTCTACT-1-s1d1                   NaN  ...    15078       34     24.8   \nGTAGAAAGTGACACAG-1-s1d1                   NaN  ...    15078       34     24.8   \nTCCGAAAAGGATCATA-1-s1d1              0.735261  ...    15078       34     24.8   \n\n                        DonorBloodType DonorRace           Ethnicity  \\\nGCATTAGCATAAGCGG-1-s1d1             B-     White  HISPANIC OR LATINO   \nTACAGGTGTTAGAGTA-1-s1d1             B-     White  HISPANIC OR LATINO   \nAGGATCTAGGTCTACT-1-s1d1             B-     White  HISPANIC OR LATINO   \nGTAGAAAGTGACACAG-1-s1d1             B-     White  HISPANIC OR LATINO   \nTCCGAAAAGGATCATA-1-s1d1             B-     White  HISPANIC OR LATINO   \n\n                         DonorGender  QCMeds  DonorSmoker is_train  \nGCATTAGCATAAGCGG-1-s1d1         Male   False    Nonsmoker    train  \nTACAGGTGTTAGAGTA-1-s1d1         Male   False    Nonsmoker    train  \nAGGATCTAGGTCTACT-1-s1d1         Male   False    Nonsmoker    train  \nGTAGAAAGTGACACAG-1-s1d1         Male   False    Nonsmoker    train  \nTCCGAAAAGGATCATA-1-s1d1         Male   False    Nonsmoker    train  \n\n[5 rows x 26 columns]\n           feature_types          gene_id\nAL627309.5           GEX  ENSG00000241860\nLINC01409            GEX  ENSG00000237491\nLINC01128            GEX  ENSG00000228794\nLINC00115            GEX  ENSG00000225880\nFAM41C               GEX  ENSG00000230368\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import scanpy as sc\nimport numpy as np\n\ndef balanced_downsample(adata, percentile=50, rare_threshold_ratio=0.5, min_cells=200, random_state=42):\n    \"\"\"\n    Downsample dataset to balance cell types without extreme reduction.\n\n    Parameters:\n    - adata: AnnData object containing single-cell data.\n    - percentile: Percentile to determine downsampling target.\n    - rare_threshold_ratio: Threshold for rare classes (relative to mean count).\n    - min_cells: Minimum number of cells per class.\n    - random_state: Seed for reproducibility.\n\n    Returns:\n    - Balanced AnnData object.\n    \"\"\"\n    np.random.seed(random_state)\n\n    # Compute stats\n    cell_counts = adata.obs[\"celltype\"].value_counts()\n    avg_count = cell_counts.mean()\n    median_count = cell_counts.median()\n    q1_count = int(np.percentile(cell_counts, percentile))\n\n    # Determine adaptive target\n    target_count = max(q1_count, median_count, int(rare_threshold_ratio * avg_count), min_cells)\n\n    # Ensure target_count is an integer\n    target_count = int(target_count)\n\n    print(f\"Avg: {avg_count:.2f}, Median: {median_count}, Q1 ({percentile}%): {q1_count}, Target: {target_count}\")\n\n    balanced_indices = []\n    for cell_type, count in cell_counts.items():\n        cell_indices = adata.obs[adata.obs[\"celltype\"] == cell_type].index\n\n        # Keep rare classes untouched, only downsample large ones\n        if count <= target_count:\n            sampled_indices = cell_indices\n        else:\n            sampled_indices = np.random.choice(cell_indices, target_count, replace=False)\n\n        balanced_indices.extend(sampled_indices)\n\n    # Create balanced dataset\n    balanced_adata = adata[balanced_indices].copy()\n    \n    print(f\"New balanced dataset shape: {balanced_adata.shape}\")\n    return balanced_adata\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:25.248017Z","iopub.execute_input":"2025-03-08T09:43:25.248267Z","iopub.status.idle":"2025-03-08T09:43:25.254726Z","shell.execute_reply.started":"2025-03-08T09:43:25.248246Z","shell.execute_reply":"2025-03-08T09:43:25.253906Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define dataset\ndataset_name = \"cite_seq\"\nif dataset_name == \"cite_seq\":\n    # data_dir = Path(\"../data/cite_seq\")\n    \n    # Load the dataset\n    adata = sc.read(\"/kaggle/input/citeseqscrnaseqproteins-challenge-neurips2021/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad\")\n    # Splitting into train and test based on 'is_train' column\n    adata_train = adata[adata.obs[\"is_train\"] == \"train\"].copy()\n    adata_test = adata[adata.obs[\"is_train\"] == \"test\"].copy()\n\n    print(adata_train.var.index.duplicated().sum())  # Number of duplicated genes in train\n    print(adata_test.var.index.duplicated().sum())  # Number of duplicated genes in test\n    adata_train = adata_train[:, ~adata_train.var.index.duplicated()].copy()\n    adata_test = adata_test[:, ~adata_test.var.index.duplicated()].copy()\n\n    # Assign cell types and batch IDs\n    adata_train.obs[\"celltype\"] = adata_train.obs[\"cell_type\"].astype(\"category\")\n    adata_test.obs[\"celltype\"] = adata_test.obs[\"cell_type\"].astype(\"category\")\n    # Encode batch IDs properly\n    adata_train.obs[\"batch_id\"] = adata_train.obs[\"is_train\"].astype(\"category\")\n    adata_test.obs[\"batch_id\"] = adata_test.obs[\"is_train\"].astype(\"category\")\n    # Convert batch to numerical encoding\n    adata_train.obs[\"str_batch\"] = adata_train.obs[\"batch_id\"].cat.codes.astype(str)\n    adata_test.obs[\"str_batch\"] = adata_test.obs[\"batch_id\"].cat.codes.astype(str)\n\n    # Align genes in train & test before merging\n    common_genes = adata_train.var.index.intersection(adata_test.var.index)\n    adata_train = adata_train[:, common_genes].copy()\n    adata_test = adata_test[:, common_genes].copy()\n\n    # Apply balancing function to train dataset\n    adata_train = balanced_downsample(adata_train)    \n    # Apply balancing function to test dataset\n    adata_test = balanced_downsample(adata_test)\n    \n    # Flags for downstream use\n    data_is_raw = False\n    filter_gene_by_counts = False\n    # Keep raw test dataset\n    adata_test_raw = adata_test.copy()\n    # Merge datasets\n    adata = adata_train.concatenate(adata_test, batch_key=\"str_batch\")\n\n# Assign batch and cell type IDs for further processing\nbatch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\nadata.obs[\"batch_id\"] = batch_id_labels\n# Store unique cell types and mapping\ncelltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\ncelltypes = adata.obs[\"celltype\"].unique()\nnum_types = len(np.unique(celltype_id_labels))\nid2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\nadata.obs[\"celltype_id\"] = celltype_id_labels\n# Store gene names\nadata.var[\"gene_id\"] = adata.var.index.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:25.255535Z","iopub.execute_input":"2025-03-08T09:43:25.255759Z","iopub.status.idle":"2025-03-08T09:43:49.447445Z","shell.execute_reply.started":"2025-03-08T09:43:25.255740Z","shell.execute_reply":"2025-03-08T09:43:49.446808Z"}},"outputs":[{"name":"stdout","text":"36\n36\n📊 Avg: 1470.56, Median: 725.0, Q1 (50%): 725, Target: 735\n✅ New balanced dataset shape: (22813, 14051)\n📊 Avg: 376.65, Median: 235.5, Q1 (50%): 235, Target: 235\n✅ New balanced dataset shape: (7291, 14051)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"if config.load_model is not None:\n    model_dir = \"/kaggle/input/scgpt_human/keras/default/1/scGPT_human\"\n    model_config_file = model_dir +\"/\"+ \"args.json\"\n    model_file = model_dir +\"/\"+ \"best_model.pt\"\n    vocab_file = model_dir +\"/\"+ \"vocab.json\"\n\n    vocab = GeneVocab.from_file(vocab_file)\n    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n    for s in special_tokens:\n        if s not in vocab:\n            vocab.append_token(s)\n\n    adata.var[\"id_in_vocab\"] = [\n        1 if gene in vocab else -1 for gene in adata.var[\"gene_id\"]\n    ]\n    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n    logger.info(\n        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n        f\"in vocabulary of size {len(vocab)}.\"\n    )\n    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n\n    # model\n    with open(model_config_file, \"r\") as f:\n        model_configs = json.load(f)\n    logger.info(\n        f\"Resume model from {model_file}, the model args will override the \"\n        f\"config {model_config_file}.\"\n    )\n    embsize = model_configs[\"embsize\"]\n    nhead = model_configs[\"nheads\"]\n    d_hid = model_configs[\"d_hid\"]\n    nlayers = model_configs[\"nlayers\"]\n    n_layers_cls = model_configs[\"n_layers_cls\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:49.450916Z","iopub.execute_input":"2025-03-08T09:43:49.451141Z","iopub.status.idle":"2025-03-08T09:43:49.718557Z","shell.execute_reply.started":"2025-03-08T09:43:49.451123Z","shell.execute_reply":"2025-03-08T09:43:49.717936Z"}},"outputs":[{"name":"stdout","text":"scGPT - INFO - match 12588/14051 genes in vocabulary of size 60697.\nscGPT - INFO - Resume model from /kaggle/input/scgpt_human/keras/default/1/scGPT_human/best_model.pt, the model args will override the config /kaggle/input/scgpt_human/keras/default/1/scGPT_human/args.json.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nimport scanpy as sc\n\n# set up the preprocessor\npreprocessor = Preprocessor(\n    use_key=\"X\",  # the key in adata.layers to use as raw data\n    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n    filter_cell_by_counts=False,  # step 2\n    normalize_total=1e4,  # 3. normalize the raw data to this sum\n    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n    result_log1p_key=\"X_log1p\",\n    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n    binning=n_bins,  # 6. bin the raw data and to what number of bins\n    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n)\n\n# Subsetting adata\nadata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\nadata = adata[adata.obs[\"str_batch\"] == \"0\"]\n\n# Preprocessing the data\npreprocessor(adata, batch_key=None)\npreprocessor(adata_test, batch_key=None)\n\n# Feature Selection: Selecting Highly Variable Genes (HVGs)\nsc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=2000, subset=True)\n\n# Handling Missing Data: Imputation using SimpleImputer\nimputer = SimpleImputer(strategy='mean')  # Or use 'median' or 'most_frequent' based on the dataset characteristics\nadata.X = imputer.fit_transform(adata.X)\n\n# Normalize features (standardize genes' expression)\nscaler = StandardScaler()\nadata.X = scaler.fit_transform(adata.X)\n\n# Run preprocessing steps for adata_test (the test batch)\npreprocessor(adata_test, batch_key=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # set up the preprocessor, use the args to config the workflow\n# preprocessor = Preprocessor(\n#     use_key=\"X\",  # the key in adata.layers to use as raw data\n#     filter_gene_by_counts=filter_gene_by_counts,  # step 1\n#     filter_cell_by_counts=False,  # step 2\n#     normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n#     result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n#     log1p=data_is_raw,  # 4. whether to log1p the normalized data\n#     result_log1p_key=\"X_log1p\",\n#     subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n#     hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n#     binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n#     result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n# )\n\n\n# adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n# adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n\n# preprocessor(adata, batch_key=None)\n# preprocessor(adata_test, batch_key=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:43:49.720002Z","iopub.execute_input":"2025-03-08T09:43:49.720207Z","iopub.status.idle":"2025-03-08T09:44:10.998444Z","shell.execute_reply.started":"2025-03-08T09:43:49.720188Z","shell.execute_reply":"2025-03-08T09:44:10.997427Z"}},"outputs":[{"name":"stdout","text":"scGPT - INFO - Normalizing total counts ...\nscGPT - INFO - Binning data ...\nscGPT - INFO - Normalizing total counts ...\nscGPT - INFO - Binning data ...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n    \"normed_raw\": \"X_normed\",\n    \"log1p\": \"X_normed\",\n    \"binned\": \"X_binned\",\n}[input_style]\nall_counts = (\n    adata.layers[input_layer_key].A\n    if issparse(adata.layers[input_layer_key])\n    else adata.layers[input_layer_key]\n)\ngenes = adata.var[\"gene_id\"].tolist()\n\ncelltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\ncelltypes_labels = np.array(celltypes_labels)\n\nbatch_ids = adata.obs[\"batch_id\"].tolist()\nnum_batch_types = len(set(batch_ids))\nbatch_ids = np.array(batch_ids)\n\n(\n    train_data,\n    valid_data,\n    train_celltype_labels,\n    valid_celltype_labels,\n    train_batch_labels,\n    valid_batch_labels,\n) = train_test_split(\n    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:10.999364Z","iopub.execute_input":"2025-03-08T09:44:10.999617Z","iopub.status.idle":"2025-03-08T09:44:12.488577Z","shell.execute_reply.started":"2025-03-08T09:44:10.999575Z","shell.execute_reply":"2025-03-08T09:44:12.487585Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"if config.load_model is None:\n    vocab = Vocab(\n        VocabPybind(genes + special_tokens, None)\n    )  # bidirectional lookup [gene <-> int]\nvocab.set_default_index(vocab[\"<pad>\"])\ngene_ids = np.array(vocab(genes), dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:12.489542Z","iopub.execute_input":"2025-03-08T09:44:12.489824Z","iopub.status.idle":"2025-03-08T09:44:12.498778Z","shell.execute_reply.started":"2025-03-08T09:44:12.489803Z","shell.execute_reply":"2025-03-08T09:44:12.497945Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"tokenized_train = tokenize_and_pad_batch(\n    train_data,\n    gene_ids,\n    max_len=max_seq_len,\n    vocab=vocab,\n    pad_token=pad_token,\n    pad_value=pad_value,\n    append_cls=True,  # append <cls> token at the beginning\n    include_zero_gene=include_zero_gene,\n)\ntokenized_valid = tokenize_and_pad_batch(\n    valid_data,\n    gene_ids,\n    max_len=max_seq_len,\n    vocab=vocab,\n    pad_token=pad_token,\n    pad_value=pad_value,\n    append_cls=True,\n    include_zero_gene=include_zero_gene,\n)\nlogger.info(\n    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n)\nlogger.info(\n    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:12.499665Z","iopub.execute_input":"2025-03-08T09:44:12.499879Z","iopub.status.idle":"2025-03-08T09:44:16.896892Z","shell.execute_reply.started":"2025-03-08T09:44:12.499860Z","shell.execute_reply":"2025-03-08T09:44:16.896058Z"}},"outputs":[{"name":"stdout","text":"scGPT - INFO - train set number of samples: 20531, \n\t feature length: 3001\nscGPT - INFO - valid set number of samples: 2282, \n\t feature length: 3001\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n    masked_values_train = random_mask_value(\n        tokenized_train[\"values\"],\n        mask_ratio=mask_ratio,\n        mask_value=mask_value,\n        pad_value=pad_value,\n    )\n    masked_values_valid = random_mask_value(\n        tokenized_valid[\"values\"],\n        mask_ratio=mask_ratio,\n        mask_value=mask_value,\n        pad_value=pad_value,\n    )\n    print(\n        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n    )\n\n    input_gene_ids_train, input_gene_ids_valid = (\n        tokenized_train[\"genes\"],\n        tokenized_valid[\"genes\"],\n    )\n    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n    target_values_train, target_values_valid = (\n        tokenized_train[\"values\"],\n        tokenized_valid[\"values\"],\n    )\n\n    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n\n    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n\n    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n        train_sort_ids = np.argsort(train_batch_labels)\n        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n        input_values_train = input_values_train[train_sort_ids]\n        target_values_train = target_values_train[train_sort_ids]\n        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n\n        valid_sort_ids = np.argsort(valid_batch_labels)\n        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n        input_values_valid = input_values_valid[valid_sort_ids]\n        target_values_valid = target_values_valid[valid_sort_ids]\n        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n\n    train_data_pt = {\n        \"gene_ids\": input_gene_ids_train,\n        \"values\": input_values_train,\n        \"target_values\": target_values_train,\n        \"batch_labels\": tensor_batch_labels_train,\n        \"celltype_labels\": tensor_celltype_labels_train,\n    }\n    valid_data_pt = {\n        \"gene_ids\": input_gene_ids_valid,\n        \"values\": input_values_valid,\n        \"target_values\": target_values_valid,\n        \"batch_labels\": tensor_batch_labels_valid,\n        \"celltype_labels\": tensor_celltype_labels_valid,\n    }\n\n    return train_data_pt, valid_data_pt\n\n\n# dataset\nclass SeqDataset(Dataset):\n    def __init__(self, data: Dict[str, torch.Tensor]):\n        self.data = data\n\n    def __len__(self):\n        return self.data[\"gene_ids\"].shape[0]\n\n    def __getitem__(self, idx):\n        return {k: v[idx] for k, v in self.data.items()}\n\n\n# data_loader\ndef prepare_dataloader(\n    data_pt: Dict[str, torch.Tensor],\n    batch_size: int,\n    shuffle: bool = False,\n    intra_domain_shuffle: bool = False,\n    drop_last: bool = False,\n    num_workers: int = 0,\n) -> DataLoader:\n    if num_workers == 0:\n        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n\n    dataset = SeqDataset(data_pt)\n\n    if per_seq_batch_sample:\n        # find the indices of samples in each seq batch\n        subsets = []\n        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n        for batch_label in np.unique(batch_labels_array):\n            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n            subsets.append(batch_indices)\n        data_loader = DataLoader(\n            dataset=dataset,\n            batch_sampler=SubsetsBatchSampler(\n                subsets,\n                batch_size,\n                intra_subset_shuffle=intra_domain_shuffle,\n                inter_subset_shuffle=shuffle,\n                drop_last=drop_last,\n            ),\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n        return data_loader\n\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    return data_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:16.897833Z","iopub.execute_input":"2025-03-08T09:44:16.898066Z","iopub.status.idle":"2025-03-08T09:44:16.909743Z","shell.execute_reply.started":"2025-03-08T09:44:16.898044Z","shell.execute_reply":"2025-03-08T09:44:16.908957Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Step 3: Load the pre-trained scGPT model","metadata":{}},{"cell_type":"markdown","source":" # For ScBERT","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import BertModel, BertTokenizer\n\n# # Detect available GPUs\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# num_gpus = torch.cuda.device_count()\n\n# # Load pre-trained scBERT (replace 'scBERT' with the actual model name, if different)\n# scbert_model_name = 'allenai/scibert_scivocab_uncased'  # You can change it based on your preference or the model you're using\n# scbert = BertModel.from_pretrained(scbert_model_name).to(device)\n# tokenizer = BertTokenizer.from_pretrained(scbert_model_name)\n\n# # Define your Transformer model (which uses the scBERT embedding as the encoder)\n# ntokens = len(vocab)  # size of vocabulary\n# model = TransformerModel(\n#     ntokens,\n#     embsize,\n#     nhead,\n#     d_hid,\n#     nlayers,\n#     nlayers_cls=3,\n#     n_cls=num_types if CLS else 1,\n#     vocab=vocab,\n#     dropout=dropout,\n#     pad_token=pad_token,\n#     pad_value=pad_value,\n#     do_mvc=MVC,\n#     do_dab=DAB,\n#     use_batch_labels=INPUT_BATCH_LABELS,\n#     num_batch_labels=num_batch_types,\n#     domain_spec_batchnorm=config.DSBN,\n#     input_emb_style=input_emb_style,\n#     n_input_bins=n_input_bins,\n#     cell_emb_style=cell_emb_style,\n#     mvc_decoder_style=mvc_decoder_style,\n#     ecs_threshold=ecs_threshold,\n#     explicit_zero_prob=explicit_zero_prob,\n#     use_fast_transformer=fast_transformer,\n#     fast_transformer_backend=fast_transformer_backend,\n#     pre_norm=config.pre_norm,\n# )\n\n# # Integrate scBERT into your model\n# class scBERTTransformerModel(nn.Module):\n#     def __init__(self, scbert, transformer_model):\n#         super(scBERTTransformerModel, self).__init__()\n#         self.scbert = scbert  # Pre-trained scBERT\n#         self.transformer = transformer_model  # Your custom transformer model\n\n#     def forward(self, input_ids, attention_mask):\n#         # Get embeddings from scBERT\n#         scbert_output = self.scbert(input_ids=input_ids, attention_mask=attention_mask)\n#         embeddings = scbert_output.last_hidden_state  # You can use the [CLS] token embeddings or the full sequence\n\n#         # Pass scBERT embeddings to the transformer model\n#         output = self.transformer(embeddings)\n#         return output\n\n# # Create an integrated model\n# scbert_transformer_model = scBERTTransformerModel(scbert, model).to(device)\n\n# # Move model to device first\n# scbert_transformer_model.to(device)\n\n# # Load model weights if specified\n# if config.load_model is not None:\n#     try:\n#         scbert_transformer_model.load_state_dict(torch.load(model_file, map_location=device))\n#         logger.info(f\"Loading all model params from {model_file}\")\n#     except:\n#         # Load matching parameters only\n#         model_dict = scbert_transformer_model.state_dict()\n#         pretrained_dict = torch.load(model_file, map_location=device)\n#         pretrained_dict = {\n#             k: v\n#             for k, v in pretrained_dict.items()\n#             if k in model_dict and v.shape == model_dict[k].shape\n#         }\n#         for k, v in pretrained_dict.items():\n#             logger.info(f\"Loading params {k} with shape {v.shape}\")\n#         model_dict.update(pretrained_dict)\n#         scbert_transformer_model.load_state_dict(model_dict)\n\n# # Enable Multi-GPU if more than 1 GPU is available\n# if num_gpus > 1:\n#     print(f\"Using {num_gpus} GPUs with DataParallel!\")\n#     scbert_transformer_model = nn.DataParallel(scbert_transformer_model)\n\n# # Freeze encoder parameters if required\n# pre_freeze_param_count = sum(\n#     dict((p.data_ptr(), p.numel()) for p in scbert_transformer_model.parameters() if p.requires_grad).values()\n# )\n\n# for name, para in scbert_transformer_model.named_parameters():\n#     print(\"-\"*20)\n#     print(f\"name: {name}\")\n#     if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n#         print(f\"Freezing weights for: {name}\")\n#         para.requires_grad = False\n\n# post_freeze_param_count = sum(\n#     dict((p.data_ptr(), p.numel()) for p in scbert_transformer_model.parameters() if p.requires_grad).values()\n# )\n\n# logger.info(f\"Total Pre freeze Params {pre_freeze_param_count}\")\n# logger.info(f\"Total Post freeze Params {post_freeze_param_count}\")\n# wandb.log(\n#     {\n#         \"info/pre_freeze_param_count\": pre_freeze_param_count,\n#         \"info/post_freeze_param_count\": post_freeze_param_count,\n#     },\n# )\n\n# # Move model again after freezing parameters\n# scbert_transformer_model.to(device)\n\n# # Enable Weights & Biases logging\n# if isinstance(scbert_transformer_model, torch.nn.DataParallel):\n#     wandb.watch(scbert_transformer_model.module)  # Access the original model inside DataParallel\n# else:\n#     wandb.watch(scbert_transformer_model)\n\n# # If adversarial training (ADV) is enabled, use multiple GPUs for the discriminator too\n# if ADV:\n#     discriminator = AdversarialDiscriminator(\n#         d_model=embsize,\n#         n_cls=num_batch_types,\n#     ).to(device)\n\n#     if num_gpus > 1:\n#         discriminator = nn.DataParallel(discriminator)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For scGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Detect available GPUs\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnum_gpus = torch.cuda.device_count()\n\n# Define model\nntokens = len(vocab)  # size of vocabulary\nmodel = TransformerModel(\n    ntokens,\n    embsize,\n    nhead,\n    d_hid,\n    nlayers,\n    nlayers_cls=3,\n    n_cls=num_types if CLS else 1,\n    vocab=vocab,\n    dropout=dropout,\n    pad_token=pad_token,\n    pad_value=pad_value,\n    do_mvc=MVC,\n    do_dab=DAB,\n    use_batch_labels=INPUT_BATCH_LABELS,\n    num_batch_labels=num_batch_types,\n    domain_spec_batchnorm=config.DSBN,\n    input_emb_style=input_emb_style,\n    n_input_bins=n_input_bins,\n    cell_emb_style=cell_emb_style,\n    mvc_decoder_style=mvc_decoder_style,\n    ecs_threshold=ecs_threshold,\n    explicit_zero_prob=explicit_zero_prob,\n    use_fast_transformer=fast_transformer,\n    fast_transformer_backend=fast_transformer_backend,\n    pre_norm=config.pre_norm,\n)\n\n# Move model to device first\nmodel.to(device)\n\n# Load model weights if specified\nif config.load_model is not None:\n    try:\n        model.load_state_dict(torch.load(model_file, map_location=device))\n        logger.info(f\"Loading all model params from {model_file}\")\n    except:\n        # Load matching parameters only\n        model_dict = model.state_dict()\n        pretrained_dict = torch.load(model_file, map_location=device)\n        pretrained_dict = {\n            k: v\n            for k, v in pretrained_dict.items()\n            if k in model_dict and v.shape == model_dict[k].shape\n        }\n        for k, v in pretrained_dict.items():\n            logger.info(f\"Loading params {k} with shape {v.shape}\")\n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n\n# Enable Multi-GPU if more than 1 GPU is available\nif num_gpus > 1:\n    print(f\"Using {num_gpus} GPUs with DataParallel!\")\n    model = nn.DataParallel(model)\n\n# Freeze encoder parameters if required\npre_freeze_param_count = sum(\n    dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values()\n)\n\nfor name, para in model.named_parameters():\n    print(\"-\"*20)\n    print(f\"name: {name}\")\n    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n        print(f\"Freezing weights for: {name}\")\n        para.requires_grad = False\n\npost_freeze_param_count = sum(\n    dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values()\n)\n\nlogger.info(f\"Total Pre freeze Params {pre_freeze_param_count}\")\nlogger.info(f\"Total Post freeze Params {post_freeze_param_count}\")\nwandb.log(\n    {\n        \"info/pre_freeze_param_count\": pre_freeze_param_count,\n        \"info/post_freeze_param_count\": post_freeze_param_count,\n    },\n)\n\n# Move model again after freezing parameters\nmodel.to(device)\n\n# # Enable Weights & Biases logging\n# wandb.watch(model)\n\n#Modify wandb.watch(model) for Multi-GPU\nif isinstance(model, torch.nn.DataParallel):\n    wandb.watch(model.module)  # Access the original model inside DataParallel\nelse:\n    wandb.watch(model)\n\n\n# If adversarial training (ADV) is enabled, use multiple GPUs for the discriminator too\nif ADV:\n    discriminator = AdversarialDiscriminator(\n        d_model=embsize,\n        n_cls=num_batch_types,\n    ).to(device)\n\n    if num_gpus > 1:\n        discriminator = nn.DataParallel(discriminator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:16.910634Z","iopub.execute_input":"2025-03-08T09:44:16.910847Z","iopub.status.idle":"2025-03-08T09:44:20.865308Z","shell.execute_reply.started":"2025-03-08T09:44:16.910827Z","shell.execute_reply":"2025-03-08T09:44:20.864470Z"}},"outputs":[{"name":"stdout","text":"scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\nscGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\nscGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\nscGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\nscGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\nscGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\nscGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\nscGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\nscGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\nscGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\nscGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\nUsing 2 GPUs with DataParallel!\n--------------------\nname: module.encoder.embedding.weight\n--------------------\nname: module.encoder.enc_norm.weight\n--------------------\nname: module.encoder.enc_norm.bias\n--------------------\nname: module.value_encoder.linear1.weight\n--------------------\nname: module.value_encoder.linear1.bias\n--------------------\nname: module.value_encoder.linear2.weight\n--------------------\nname: module.value_encoder.linear2.bias\n--------------------\nname: module.value_encoder.norm.weight\n--------------------\nname: module.value_encoder.norm.bias\n--------------------\nname: module.transformer_encoder.layers.0.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.0.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.0.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.0.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.0.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.0.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.0.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.0.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.0.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.0.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.0.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.0.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.1.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.1.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.1.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.1.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.1.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.1.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.1.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.1.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.1.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.1.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.1.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.1.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.2.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.2.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.2.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.2.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.2.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.2.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.2.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.2.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.2.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.2.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.2.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.2.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.3.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.3.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.3.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.3.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.3.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.3.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.3.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.3.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.3.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.3.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.3.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.3.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.4.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.4.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.4.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.4.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.4.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.4.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.4.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.4.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.4.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.4.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.4.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.4.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.5.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.5.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.5.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.5.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.5.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.5.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.5.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.5.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.5.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.5.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.5.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.5.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.6.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.6.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.6.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.6.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.6.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.6.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.6.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.6.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.6.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.6.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.6.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.6.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.7.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.7.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.7.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.7.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.7.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.7.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.7.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.7.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.7.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.7.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.7.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.7.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.8.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.8.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.8.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.8.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.8.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.8.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.8.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.8.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.8.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.8.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.8.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.8.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.9.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.9.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.9.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.9.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.9.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.9.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.9.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.9.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.9.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.9.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.9.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.9.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.10.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.10.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.10.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.10.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.10.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.10.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.10.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.10.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.10.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.10.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.10.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.10.norm2.bias\n--------------------\nname: module.transformer_encoder.layers.11.self_attn.in_proj_weight\n--------------------\nname: module.transformer_encoder.layers.11.self_attn.in_proj_bias\n--------------------\nname: module.transformer_encoder.layers.11.self_attn.out_proj.weight\n--------------------\nname: module.transformer_encoder.layers.11.self_attn.out_proj.bias\n--------------------\nname: module.transformer_encoder.layers.11.linear1.weight\n--------------------\nname: module.transformer_encoder.layers.11.linear1.bias\n--------------------\nname: module.transformer_encoder.layers.11.linear2.weight\n--------------------\nname: module.transformer_encoder.layers.11.linear2.bias\n--------------------\nname: module.transformer_encoder.layers.11.norm1.weight\n--------------------\nname: module.transformer_encoder.layers.11.norm1.bias\n--------------------\nname: module.transformer_encoder.layers.11.norm2.weight\n--------------------\nname: module.transformer_encoder.layers.11.norm2.bias\n--------------------\nname: module.decoder.fc.0.weight\n--------------------\nname: module.decoder.fc.0.bias\n--------------------\nname: module.decoder.fc.2.weight\n--------------------\nname: module.decoder.fc.2.bias\n--------------------\nname: module.decoder.fc.4.weight\n--------------------\nname: module.decoder.fc.4.bias\n--------------------\nname: module.cls_decoder._decoder.0.weight\n--------------------\nname: module.cls_decoder._decoder.0.bias\n--------------------\nname: module.cls_decoder._decoder.2.weight\n--------------------\nname: module.cls_decoder._decoder.2.bias\n--------------------\nname: module.cls_decoder._decoder.3.weight\n--------------------\nname: module.cls_decoder._decoder.3.bias\n--------------------\nname: module.cls_decoder._decoder.5.weight\n--------------------\nname: module.cls_decoder._decoder.5.bias\n--------------------\nname: module.cls_decoder.out_layer.weight\n--------------------\nname: module.cls_decoder.out_layer.bias\nscGPT - INFO - Total Pre freeze Params 51354670\nscGPT - INFO - Total Post freeze Params 51354670\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# For SingleCellNet","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import BertModel, BertTokenizer\n\n# # Detect available GPUs\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# num_gpus = torch.cuda.device_count()\n\n# # Load pre-trained scBERT (as before)\n# scbert_model_name = 'allenai/scibert_scivocab_uncased'  # Change to your desired scBERT model\n# scbert = BertModel.from_pretrained(scbert_model_name).to(device)\n# tokenizer = BertTokenizer.from_pretrained(scbert_model_name)\n\n# # Load SingleCellNet model (assuming it's a PyTorch model)\n# # You need to modify this part based on how SingleCellNet is saved or loaded.\n# # If it's available as a PyTorch model, load it similarly to scBERT\n\n# # Assuming SingleCellNet is a PyTorch model (this could be different if it's TensorFlow or another framework)\n# singlecellnet_model_path = \"path/to/singlecellnet_model.pth\"  # Path to the pre-trained SingleCellNet model file\n# singlecellnet_model = torch.load(singlecellnet_model_path, map_location=device)\n\n# # Example SingleCellNet Model Architecture (this will vary based on the model's actual definition)\n# class SingleCellNet(nn.Module):\n#     def __init__(self):\n#         super(SingleCellNet, self).__init__()\n#         # Define the architecture (replace with the actual model details)\n#         self.layer1 = nn.Linear(512, 256)\n#         self.layer2 = nn.Linear(256, 128)\n#         self.output_layer = nn.Linear(128, 10)  # Example for 10 classes\n\n#     def forward(self, x):\n#         x = torch.relu(self.layer1(x))\n#         x = torch.relu(self.layer2(x))\n#         x = self.output_layer(x)\n#         return x\n\n# # Assuming the above class is the correct architecture, replace with actual loading if different\n# singlecellnet = SingleCellNet().to(device)\n\n# # Load pre-trained weights if available\n# singlecellnet.load_state_dict(torch.load(singlecellnet_model_path, map_location=device))\n\n# # Define your Transformer model as before\n# ntokens = len(vocab)  # size of vocabulary\n# model = TransformerModel(\n#     ntokens,\n#     embsize,\n#     nhead,\n#     d_hid,\n#     nlayers,\n#     nlayers_cls=3,\n#     n_cls=num_types if CLS else 1,\n#     vocab=vocab,\n#     dropout=dropout,\n#     pad_token=pad_token,\n#     pad_value=pad_value,\n#     do_mvc=MVC,\n#     do_dab=DAB,\n#     use_batch_labels=INPUT_BATCH_LABELS,\n#     num_batch_labels=num_batch_types,\n#     domain_spec_batchnorm=config.DSBN,\n#     input_emb_style=input_emb_style,\n#     n_input_bins=n_input_bins,\n#     cell_emb_style=cell_emb_style,\n#     mvc_decoder_style=mvc_decoder_style,\n#     ecs_threshold=ecs_threshold,\n#     explicit_zero_prob=explicit_zero_prob,\n#     use_fast_transformer=fast_transformer,\n#     fast_transformer_backend=fast_transformer_backend,\n#     pre_norm=config.pre_norm,\n# )\n\n# # Combine scBERT and SingleCellNet into a unified model, if necessary\n# class UnifiedModel(nn.Module):\n#     def __init__(self, scbert, singlecellnet, transformer_model):\n#         super(UnifiedModel, self).__init__()\n#         self.scbert = scbert  # Pre-trained scBERT\n#         self.singlecellnet = singlecellnet  # Pre-trained SingleCellNet\n#         self.transformer = transformer_model  # Custom transformer model\n\n#     def forward(self, input_ids, attention_mask, cell_features):\n#         # Get embeddings from scBERT\n#         scbert_output = self.scbert(input_ids=input_ids, attention_mask=attention_mask)\n#         embeddings = scbert_output.last_hidden_state  # Use the [CLS] token embeddings or the full sequence\n\n#         # Pass scBERT embeddings to the transformer model\n#         transformer_output = self.transformer(embeddings)\n\n#         # Optionally, use SingleCellNet for further classification\n#         cell_classification = self.singlecellnet(cell_features)\n\n#         return transformer_output, cell_classification\n\n# # Create an integrated model\n# unified_model = UnifiedModel(scbert, singlecellnet, model).to(device)\n\n# # Move model to device first\n# unified_model.to(device)\n\n# # Load model weights if specified\n# if config.load_model is not None:\n#     try:\n#         unified_model.load_state_dict(torch.load(model_file, map_location=device))\n#         logger.info(f\"Loading all model params from {model_file}\")\n#     except:\n#         # Load matching parameters only\n#         model_dict = unified_model.state_dict()\n#         pretrained_dict = torch.load(model_file, map_location=device)\n#         pretrained_dict = {\n#             k: v\n#             for k, v in pretrained_dict.items()\n#             if k in model_dict and v.shape == model_dict[k].shape\n#         }\n#         for k, v in pretrained_dict.items():\n#             logger.info(f\"Loading params {k} with shape {v.shape}\")\n#         model_dict.update(pretrained_dict)\n#         unified_model.load_state_dict(model_dict)\n\n# # Enable Multi-GPU if more than 1 GPU is available\n# if num_gpus > 1:\n#     print(f\"Using {num_gpus} GPUs with DataParallel!\")\n#     unified_model = nn.DataParallel(unified_model)\n\n# # Freeze encoder parameters if required\n# pre_freeze_param_count = sum(\n#     dict((p.data_ptr(), p.numel()) for p in unified_model.parameters() if p.requires_grad).values()\n# )\n\n# for name, para in unified_model.named_parameters():\n#     print(\"-\"*20)\n#     print(f\"name: {name}\")\n#     if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n#         print(f\"Freezing weights for: {name}\")\n#         para.requires_grad = False\n\n# post_freeze_param_count = sum(\n#     dict((p.data_ptr(), p.numel()) for p in unified_model.parameters() if p.requires_grad).values()\n# )\n\n# logger.info(f\"Total Pre freeze Params {pre_freeze_param_count}\")\n# logger.info(f\"Total Post freeze Params {post_freeze_param_count}\")\n# wandb.log(\n#     {\n#         \"info/pre_freeze_param_count\": pre_freeze_param_count,\n#         \"info/post_freeze_param_count\": post_freeze_param_count,\n#     },\n# )\n\n# # Move model again after freezing parameters\n# unified_model.to(device)\n\n# # Enable Weights & Biases logging\n# if isinstance(unified_model, torch.nn.DataParallel):\n#     wandb.watch(unified_model.module)  # Access the original model inside DataParallel\n# else:\n#     wandb.watch(unified_model)\n\n# # If adversarial training (ADV) is enabled, use multiple GPUs for the discriminator too\n# if ADV:\n#     discriminator = AdversarialDiscriminator(\n#         d_model=embsize,\n#         n_cls=num_batch_types,\n#     ).to(device)\n\n#     if num_gpus > 1:\n#         discriminator = nn.DataParallel(discriminator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T15:02:33.918848Z","iopub.execute_input":"2025-03-08T15:02:33.919192Z","iopub.status.idle":"2025-03-08T15:02:33.924461Z","shell.execute_reply.started":"2025-03-08T15:02:33.919159Z","shell.execute_reply":"2025-03-08T15:02:33.923515Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"criterion = masked_mse_loss\ncriterion_cls = nn.CrossEntropyLoss()\ncriterion_dab = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n)\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer, schedule_interval, gamma=config.schedule_ratio\n)\nif DAB_separate_optim:\n    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n    )\nif ADV:\n    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n    scheduler_E = torch.optim.lr_scheduler.StepLR(\n        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n    )\n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n    scheduler_D = torch.optim.lr_scheduler.StepLR(\n        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n    )\n\nscaler = torch.cuda.amp.GradScaler(enabled=config.amp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:20.866421Z","iopub.execute_input":"2025-03-08T09:44:20.866730Z","iopub.status.idle":"2025-03-08T09:44:21.779947Z","shell.execute_reply.started":"2025-03-08T09:44:20.866692Z","shell.execute_reply":"2025-03-08T09:44:21.778826Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train(model: nn.Module, loader: DataLoader) -> None:\n    \"\"\"\n    Train the model for one epoch.\n    \"\"\"\n    model.train()\n    (\n        total_loss,\n        total_mse,\n        total_cls,\n        total_cce,\n        total_mvc,\n        total_ecs,\n        total_dab,\n        total_adv_E,\n        total_adv_D,\n        total_zero_log_prob,\n        total_mvc_zero_log_prob,\n    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n    total_error = 0.0\n    start_time = time.time()\n\n    num_batches = len(loader)\n\n    # Define the number of gradient accumulation steps\n    gradient_accumulation_steps = 2  # Change this as needed\n    total_loss=0.0\n    # # Initialize the accumulator\n    # optimizer.zero_grad()\n\n    accumulation_step = 0  # Initialize for each epoch\n    for batch, batch_data in enumerate(loader):\n\n        # Initialize accumulation_step counter at the start of training or batch loop\n       \n        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n        input_values = batch_data[\"values\"].to(device)\n        target_values = batch_data[\"target_values\"].to(device)\n        batch_labels = batch_data[\"batch_labels\"].to(device)\n        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n\n        \n        # Check If GPU Memory Is Utilized Properly\n        print(\"Check If GPU Memory Is Utilized Properly\")\n        #before forward pass\n        for i in range(torch.cuda.device_count()):\n            print(f\"GPU {i}: Memory Allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n\n\n        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n        with torch.cuda.amp.autocast(enabled=config.amp):\n            output_dict = model(\n                input_gene_ids,\n                input_values,\n                src_key_padding_mask=src_key_padding_mask,\n                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n                CLS=CLS,\n                CCE=CCE,\n                MVC=MVC,\n                ECS=ECS,\n                do_sample=do_sample_in_train,\n                #generative_training=False\n            )\n\n            masked_positions = input_values.eq(mask_value)  # the postions to predict\n            loss = 0.0\n            metrics_to_log = {}\n            if MLM:\n                loss_mse = criterion(\n                    output_dict[\"mlm_output\"], target_values, masked_positions\n                )\n                loss = loss + loss_mse\n                metrics_to_log = {\"train/mse\": loss_mse.item()}\n            if explicit_zero_prob:\n                loss_zero_log_prob = criterion_neg_log_bernoulli(\n                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n                )\n                loss = loss + loss_zero_log_prob\n                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n            if CLS:\n                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n                loss = loss + loss_cls\n                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n\n                error_rate = 1 - (\n                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n                    .sum()\n                    .item()\n                ) / celltype_labels.size(0)\n            if CCE:\n                loss_cce = 10 * output_dict[\"loss_cce\"]\n                loss = loss + loss_cce\n                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n            if MVC:\n                loss_mvc = criterion(\n                    output_dict[\"mvc_output\"], target_values, masked_positions\n                )\n                loss = loss + loss_mvc\n                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n            if MVC and explicit_zero_prob:\n                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n                )\n                loss = loss + loss_mvc_zero_log_prob\n                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n            if ECS:\n                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n                loss = loss + loss_ecs\n                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n            if DAB:\n                # try weighting and separate optimizer\n                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n                loss = loss + dab_weight * loss_dab\n                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n\n        # # Gradient accumulation\n        # loss = loss / gradient_accumulation_steps  # Scale loss to accumulate gradients\n        # scaler.scale(loss).backward()\n\n        # model.zero_grad()\n\n        # Accumulate gradients (without updating the weights)\n        loss = loss / gradient_accumulation_steps  # Divide by accumulation steps to average loss\n        scaler.scale(loss).backward()\n        # scaler.unscale_(optimizer)\n\n        # Increment the accumulation step counter\n        accumulation_step += 1\n\n        # Perform optimization step after the desired number of steps\n        if (batch + 1) % gradient_accumulation_steps == 0:\n            accumulation_step = 0\n            scaler.unscale_(optimizer)\n            with warnings.catch_warnings(record=True) as w:\n                warnings.filterwarnings(\"always\")\n                torch.nn.utils.clip_grad_norm_(\n                    model.parameters(),\n                    1.0,\n                    error_if_nonfinite=False if scaler.is_enabled() else True,\n                )\n                if len(w) > 0:\n                    logger.warning(\n                        f\"Found infinite gradient. This may be caused by the gradient \"\n                        f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n                        \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n                    )\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()  # Reset gradients after optimization step\n\n            # Log progress after accumulation step\n            print(f\"Accumulation Step: {accumulation_step}/{len(loader) // gradient_accumulation_steps}, Loss: {loss.item():.4f}\")\n            logger.info(f\"Accumulation Step: {accumulation_step}/{len(loader) // gradient_accumulation_steps}, Loss: {loss.item():.4f}\")\n\n            #model.xero_grad() not used here but works \n        \n        # with warnings.catch_warnings(record=True) as w:\n        #     warnings.filterwarnings(\"always\")\n        #     torch.nn.utils.clip_grad_norm_(\n        #         model.parameters(),\n        #         1.0,\n        #         error_if_nonfinite=False if scaler.is_enabled() else True,\n        #     )\n        #     if len(w) > 0:\n        #         logger.warning(\n        #             f\"Found infinite gradient. This may be caused by the gradient \"\n        #             f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n        #             \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n        #         )\n        # scaler.step(optimizer)\n        # scaler.update()\n\n        if ADV:\n            # rerun the model for adversarial training\n            output_dict = model(\n                input_gene_ids,\n                input_values,\n                src_key_padding_mask=src_key_padding_mask,\n                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n                CLS=CLS,\n                CCE=CCE,\n                MVC=MVC,\n                ECS=ECS,\n                do_sample=do_sample_in_train,\n                #generative_training=False\n            )\n\n            # TRAINING DISCRIMINATOR\n            loss_adv_D = criterion_adv(\n                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n            )\n            if epoch > adv_D_delay_epochs:\n                discriminator.zero_grad()\n                loss_adv_D.backward()\n                optimizer_D.step()\n\n            # TRAINING ENCODER\n            loss_adv_E = -criterion_adv(\n                discriminator(output_dict[\"cell_emb\"]), batch_labels\n            )\n            # NOTE: the loss is negative here because we want to maximize\n            # the cross_entropy_loss, in other words, disguise against the discriminator\n            if epoch > adv_E_delay_epochs:\n                model.zero_grad()\n                discriminator.zero_grad()\n                loss_adv_E.backward()\n                optimizer_E.step()\n\n        wandb.log(metrics_to_log)\n\n        total_loss += loss.item()\n        total_mse += loss_mse.item() if MLM else 0.0\n        total_cls += loss_cls.item() if CLS else 0.0\n        total_cce += loss_cce.item() if CCE else 0.0\n        total_mvc += loss_mvc.item() if MVC else 0.0\n        total_ecs += loss_ecs.item() if ECS else 0.0\n        total_dab += loss_dab.item() if DAB else 0.0\n        total_adv_E += loss_adv_E.item() if ADV else 0.0\n        total_adv_D += loss_adv_D.item() if ADV else 0.0\n        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n        total_mvc_zero_log_prob += (\n            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n        )\n        total_error += error_rate\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            cur_mse = total_mse / log_interval\n            cur_cls = total_cls / log_interval if CLS else 0.0\n            cur_cce = total_cce / log_interval if CCE else 0.0\n            cur_mvc = total_mvc / log_interval if MVC else 0.0\n            cur_ecs = total_ecs / log_interval if ECS else 0.0\n            cur_dab = total_dab / log_interval if DAB else 0.0\n            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n            cur_zero_log_prob = (\n                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n            )\n            cur_mvc_zero_log_prob = (\n                total_mvc_zero_log_prob / log_interval\n                if MVC and explicit_zero_prob\n                else 0.0\n            )\n            cur_error = total_error / log_interval\n            # ppl = math.exp(cur_loss)\n            logger.info(\n                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n                f\"loss {cur_loss:5.2f} | \"\n                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n                + (\n                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n                    if MVC and explicit_zero_prob\n                    else \"\"\n                )\n            )\n            total_loss = 0\n            total_mse = 0\n            total_cls = 0\n            total_cce = 0\n            total_mvc = 0\n            total_ecs = 0\n            total_dab = 0\n            total_adv_E = 0\n            total_adv_D = 0\n            total_zero_log_prob = 0\n            total_mvc_zero_log_prob = 0\n            total_error = 0\n            start_time = time.time()\n\n\ndef define_wandb_metrcis():\n    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n    \"\"\"\n    Evaluate the model on the evaluation data.\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_error = 0.0\n    total_dab = 0.0\n    total_num = 0\n    predictions = []\n    with torch.no_grad():\n        for batch_data in loader:\n            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n            input_values = batch_data[\"values\"].to(device)\n            target_values = batch_data[\"target_values\"].to(device)\n            batch_labels = batch_data[\"batch_labels\"].to(device)\n            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n\n            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n            with torch.cuda.amp.autocast(enabled=config.amp):\n                output_dict = model(\n                    input_gene_ids,\n                    input_values,\n                    src_key_padding_mask=src_key_padding_mask,\n                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n                    CLS=CLS,  # evaluation does not need CLS or CCE\n                    CCE=False,\n                    MVC=False,\n                    ECS=False,\n                    do_sample=do_sample_in_train,\n                    #generative_training = False,\n                )\n                output_values = output_dict[\"cls_output\"]\n                loss = criterion_cls(output_values, celltype_labels)\n\n                if DAB:\n                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n\n            total_loss += loss.item() * len(input_gene_ids)\n            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n            total_num += len(input_gene_ids)\n            preds = output_values.argmax(1).cpu().numpy()\n            predictions.append(preds)\n\n    wandb.log(\n        {\n            \"valid/mse\": total_loss / total_num,\n            \"valid/err\": total_error / total_num,\n            \"valid/dab\": total_dab / total_num,\n            \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n            \"epoch\": epoch,\n        },\n    )\n\n    if return_raw:\n        return np.concatenate(predictions, axis=0)\n\n    return total_loss / total_num, total_error / total_num\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:44:21.781274Z","iopub.execute_input":"2025-03-08T09:44:21.781966Z","iopub.status.idle":"2025-03-08T09:44:21.818956Z","shell.execute_reply.started":"2025-03-08T09:44:21.781924Z","shell.execute_reply":"2025-03-08T09:44:21.817921Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Step 4: Finetune scGPT with task-specific objectives","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n# Initialize list to track epoch times for progress estimation\nepoch_times = []\n\nbest_val_loss = float(\"inf\")\nbest_avg_bio = 0.0\nbest_model = None\ndefine_wandb_metrcis()\n\n# for epoch in range(1, epochs + 1):\nfor epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\", unit=\"epoch\"):\n    epoch_start_time = time.time()\n    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n    train_loader = prepare_dataloader(\n        train_data_pt,\n        batch_size=batch_size,\n        shuffle=False,\n        intra_domain_shuffle=True,\n        drop_last=False,\n    )\n    valid_loader = prepare_dataloader(\n        valid_data_pt,\n        batch_size=eval_batch_size,\n        shuffle=False,\n        intra_domain_shuffle=False,\n        drop_last=False,\n    )\n\n    if config.do_train:\n        train(\n            model,\n            loader=train_loader,\n        )\n    val_loss, val_err = evaluate(\n        model,\n        loader=valid_loader,\n    )\n    \n    elapsed = time.time() - epoch_start_time\n    epoch_times.append(elapsed)\n    \n    logger.info(\"-\" * 89)\n    logger.info(\n        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n    )\n    \n        # Estimate remaining time\n    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n    remaining_epochs = epochs - epoch\n    total_remaining_time = avg_epoch_time * remaining_epochs\n    total_estimated_time = sum(epoch_times) + total_remaining_time\n\n    logger.info(\n        f\"Estimated time remaining: {remaining_epochs * avg_epoch_time / 60:5.2f} min | \"\n        f\"Total estimated time: {total_estimated_time / 3600:5.2f} hours\"\n    )\n\n    logger.info(\"-\" * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = copy.deepcopy(model)\n        best_model_epoch = epoch\n        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n\n    scheduler.step()\n    if DAB_separate_optim:\n        scheduler_dab.step()\n    if ADV:\n        scheduler_D.step()\n        scheduler_E.step()","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-03-08T10:29:53.638074Z","shell.execute_reply.started":"2025-03-08T09:44:21.820428Z","shell.execute_reply":"2025-03-08T10:29:53.636825Z"}},"outputs":[{"name":"stdout","text":"Check If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.9509\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.9509\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.9108\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.9108\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.9058\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.9058\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.8982\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.8982\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.6655\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.6655\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\nAccumulation Step: 0/1283, Loss: 1.8867\nscGPT - INFO - Accumulation Step: 0/1283, Loss: 1.8867\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.07 GB\nGPU 1: Memory Allocated: 0.08 GB\nCheck If GPU Memory Is Utilized Properly\nGPU 0: Memory Allocated: 1.27 GB\nGPU 1: Memory Allocated: 0.08 GB\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  50%|█████     | 1/2 [45:31<45:31, 2731.47s/epoch]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b33db71162a3>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         train(\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-611c1cee17ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    144\u001b[0m                         \u001b[0;34m\"can be ignored if no longer occurs after autoscaling of the scaler.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     )\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset gradients after optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"# torch.save(best_model.state_dict(), \"/kaggle/working/best_model_1100.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:13:21.421708Z","iopub.execute_input":"2025-03-08T12:13:21.421908Z","iopub.status.idle":"2025-03-08T12:13:21.495036Z","shell.execute_reply.started":"2025-03-08T12:13:21.421888Z","shell.execute_reply":"2025-03-08T12:13:21.493994Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c3d4d6932522>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/kaggle/working/best_model_1100.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# model.load_state_dict(torch.load(\"best_model_0700.pth\"))\n# model.eval()  # Set to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.639950Z","iopub.status.idle":"2025-03-08T10:29:53.640300Z","shell.execute_reply":"2025-03-08T10:29:53.640179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Debug GPU Memory Usage\n# import torch\n# print(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.640982Z","iopub.status.idle":"2025-03-08T10:29:53.641226Z","shell.execute_reply":"2025-03-08T10:29:53.641123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% inference\ndef test(model: nn.Module, adata: DataLoader) -> float:\n    all_counts = (\n        adata.layers[input_layer_key].A\n        if issparse(adata.layers[input_layer_key])\n        else adata.layers[input_layer_key]\n    )\n\n    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n    celltypes_labels = np.array(celltypes_labels)\n\n    batch_ids = adata.obs[\"batch_id\"].tolist()\n    batch_ids = np.array(batch_ids)\n\n    tokenized_test = tokenize_and_pad_batch(\n        all_counts,\n        gene_ids,\n        max_len=max_seq_len,\n        vocab=vocab,\n        pad_token=pad_token,\n        pad_value=pad_value,\n        append_cls=True,  # append <cls> token at the beginning\n        include_zero_gene=include_zero_gene,\n    )\n\n    input_values_test = random_mask_value(\n        tokenized_test[\"values\"],\n        mask_ratio=mask_ratio,\n        mask_value=mask_value,\n        pad_value=pad_value,\n    )\n\n    test_data_pt = {\n        \"gene_ids\": tokenized_test[\"genes\"],\n        \"values\": input_values_test,\n        \"target_values\": tokenized_test[\"values\"],\n        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n    }\n\n    test_loader = DataLoader(\n        dataset=SeqDataset(test_data_pt),\n        batch_size=eval_batch_size,\n        shuffle=False,\n        drop_last=False,\n        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n        pin_memory=True,\n    )\n\n    model.eval()\n    predictions = evaluate(\n        model,\n        loader=test_loader,\n        return_raw=True,\n    )\n\n    # compute accuracy, precision, recall, f1\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    accuracy = accuracy_score(celltypes_labels, predictions)\n    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n\n    logger.info(\n        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n        f\"Macro F1: {macro_f1:.3f}\"\n    )\n\n    results = {\n        \"test/accuracy\": accuracy,\n        \"test/precision\": precision,\n        \"test/recall\": recall,\n        \"test/macro_f1\": macro_f1,\n    }\n\n    return predictions, celltypes_labels, results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.641817Z","iopub.status.idle":"2025-03-08T10:29:53.642124Z","shell.execute_reply":"2025-03-08T10:29:53.642009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Inference with fine-tuned scGPT model\nIn the cell-type annotation task, the fine-tuned scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics. Here we visualize the predicted labels over the scGPT cell embeddings, and present the confusion matrix for detailed classification performance on the cell-group level.","metadata":{}},{"cell_type":"code","source":"# import time\n# import tqdm\n# import pickle\n# import wandb\n# import scanpy as sc\n# import matplotlib.pyplot as plt\n\n# # Start time for estimation\n# test_start_time = time.time()\n\n# # Reduce test size by a factor of 10\n# total_test_samples = len(adata_test) // 10\n# adata_test_subset = adata_test[:total_test_samples]  # Select only 10% of data\n\n# # Run inference with tqdm progress bar\n# predictions, labels, results = [], [], {}\n# for i in tqdm.tqdm(range(total_test_samples), desc=\"Testing\", unit=\"sample\"):\n#     pred, label, res = test(best_model, adata_test_subset[i])  # Assuming test() handles single sample\n#     predictions.append(pred)\n#     labels.append(label)\n#     results = res  # Assuming results structure is updated per sample\n\n# # End time and compute time per sample\n# elapsed_time = time.time() - test_start_time\n# avg_time_per_sample = elapsed_time / total_test_samples\n# estimated_total_time = avg_time_per_sample * total_test_samples\n\n# # Store predictions in AnnData object\n# adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n\n# # Define color palette\n# palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n# palette_ = palette_ * 3  # Extend palette\n# palette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n\n# # Plot results\n# with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n#     sc.pl.umap(\n#         adata_test_raw,\n#         color=[\"celltype\", \"predictions\"],\n#         palette=palette_,\n#         show=False,\n#     )\n#     plt.savefig(save_dir / \"results.png\", dpi=300)\n\n# # Save results\n# save_dict = {\n#     \"predictions\": predictions,\n#     \"labels\": labels,\n#     \"results\": results,\n#     \"id_maps\": id2type\n# }\n# with open(save_dir / \"results.pkl\", \"wb\") as f:\n#     pickle.dump(save_dict, f)\n\n# # Log results to wandb\n# results[\"test/cell_umap\"] = wandb.Image(\n#     str(save_dir / \"results.png\"),\n#     caption=f\"Predictions Macro F1: {results['test/macro_f1']:.3f}\",\n# )\n# results[\"test/estimated_time_min\"] = estimated_total_time / 60  # Convert to minutes\n\n# wandb.log(results)\n\n# # Print estimated time\n# print(f\"Testing completed in {elapsed_time:.2f} seconds\")\n# print(f\"Estimated total testing time: {estimated_total_time / 60:.2f} minutes\")\n# print(f\"Reduced test set size: {total_test_samples} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.643340Z","iopub.status.idle":"2025-03-08T10:29:53.643634Z","shell.execute_reply":"2025-03-08T10:29:53.643516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(adata_test_subset.shape)  # Check the number of features\nprint(len(gene_ids))  # Check the number of gene IDs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.644523Z","iopub.status.idle":"2025-03-08T10:29:53.644812Z","shell.execute_reply":"2025-03-08T10:29:53.644701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Print first few gene names from adata_test_subset\n# print(adata_test_subset.var_names[:10])\n\n# # Print first few gene_ids\n# print(gene_ids[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.645560Z","iopub.status.idle":"2025-03-08T10:29:53.645936Z","shell.execute_reply":"2025-03-08T10:29:53.645823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check how many genes in adata_test_subset match gene_ids\n# matching_genes = [gene for gene in adata_test_subset.var_names if gene in gene_ids]\n# print(f\"Number of matching genes: {len(matching_genes)}\")\n\n# # Check a few unmatched genes\n# unmatched_genes = [gene for gene in adata_test_subset.var_names if gene not in gene_ids]\n# print(f\"Some unmatched genes: {unmatched_genes[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.647825Z","iopub.status.idle":"2025-03-08T10:29:53.648146Z","shell.execute_reply":"2025-03-08T10:29:53.648026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Filter adata_test_subset to only include the genes that are in gene_ids\n# filtered_genes = [gene for gene in adata_test_subset.var_names if gene in gene_ids]\n\n# # Subset the data to only include the valid genes\n# adata_test_subset_filtered = adata_test_subset[:, filtered_genes].copy()\n\n# # Check the new shape\n# print(adata_test_subset_filtered.shape)  # This should have 12,588 features now\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.648883Z","iopub.status.idle":"2025-03-08T10:29:53.649198Z","shell.execute_reply":"2025-03-08T10:29:53.649081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(gene_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.650098Z","iopub.status.idle":"2025-03-08T10:29:53.650461Z","shell.execute_reply":"2025-03-08T10:29:53.650344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Reduce test size by a factor of 10\n# total_test_samples = len(adata_test) // 10\n# adata_test_subset = adata_test[:total_test_samples]  # Select only 10% of data\n\npredictions, labels, results = test(best_model, adata_test_subset)\nadata_test_raw_subset = adata_test_raw[:total_test_samples]\nadata_test_raw_subset.obs[\"predictions\"] = [id2type[int(p)] for p in predictions]\n\n# plot\npalette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \npalette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\npalette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n\nwith plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n    sc.pl.umap(\n        adata_test_raw,\n        color=[\"celltype\", \"predictions\"],\n        palette=palette_,\n        show=False,\n    )\n    plt.savefig(save_dir / \"results.png\", dpi=300)\n\nsave_dict = {\n    \"predictions\": predictions,\n    \"labels\": labels,\n    \"results\": results,\n    \"id_maps\": id2type\n}\nwith open(save_dir / \"results.pkl\", \"wb\") as f:\n    pickle.dump(save_dict, f)\n\nresults[\"test/cell_umap\"] = wandb.Image(\n    str(save_dir / \"results.png\"),\n    caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n)\nwandb.log(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.651072Z","iopub.status.idle":"2025-03-08T10:29:53.651400Z","shell.execute_reply":"2025-03-08T10:29:53.651238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncelltypes = list(celltypes)\nfor i in set([id2type[p] for p in predictions]):\n    if i not in celltypes:\n        celltypes.remove(i)\ncm = confusion_matrix(labels, predictions)\ncm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\ncm = pd.DataFrame(cm, index=celltypes[:cm.shape[0]], columns=celltypes[:cm.shape[1]])\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\nplt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n\nresults[\"test/confusion_matrix\"] = wandb.Image(\n    str(save_dir / \"confusion_matrix.png\"),\n    caption=f\"confusion matrix\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.652466Z","iopub.status.idle":"2025-03-08T10:29:53.652825Z","shell.execute_reply":"2025-03-08T10:29:53.652659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save the model into the save_dir\ntorch.save(best_model.state_dict(),\"/kaggle/working/model-0700.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T10:29:53.653758Z","iopub.status.idle":"2025-03-08T10:29:53.654121Z","shell.execute_reply":"2025-03-08T10:29:53.653963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, adjusted_rand_score, normalized_mutual_info_score\n\n# Load datasets based on the name and source type\ndef load_dataset(dataset_name, data_dir):\n    \"\"\"\n    Load scRNA-seq dataset based on the name and data directory.\n    \n    Parameters:\n    - dataset_name: str, name of the dataset to load ('covid19', 'healthy_lung', 'lung_cancer').\n    - data_dir: str, the directory where the dataset files are stored.\n    \n    Returns:\n    - Anndata object containing the dataset.\n    \"\"\"\n    # Ensure the directory exists\n    if not os.path.exists(data_dir):\n        raise ValueError(f\"Data directory {data_dir} does not exist.\")\n    if dataset_name == \"covid19\":\n        covid19_file = os.path.join(data_dir, \"covid19_data.h5\")\n        if os.path.exists(covid19_file):\n            data = sc.read_10x_h5(covid19_file)\n        else:\n            raise ValueError(\"COVID-19 dataset not found.\")\n    elif dataset_name == \"healthy_lung\":\n        healthy_lung_file = os.path.join(data_dir, \"healthy_lung_data.h5\")\n        if os.path.exists(healthy_lung_file):\n            data = sc.read_10x_h5(healthy_lung_file)\n        else:\n            raise ValueError(\"Healthy Lung Tissue dataset not found.\")\n    elif dataset_name == \"lung_cancer\":\n        lung_cancer_file = os.path.join(data_dir, \"lung_cancer_data.h5\")\n        if os.path.exists(lung_cancer_file):\n            data = sc.read_10x_h5(lung_cancer_file)\n        else:\n            raise ValueError(\"Lung Cancer dataset not found.\")\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not recognized.\")\n    # Return the loaded dataset (AnnData object)\n    return data\n\n# Preprocessing function\ndef preprocess_data(adata):\n    \"\"\"\n    Preprocess the scRNA-seq data: filtering, normalization, log transformation.\n    \n    Parameters:\n    - adata: AnnData object containing the raw scRNA-seq data.\n    \n    Returns:\n    - adata: Processed AnnData object.\n    \"\"\"\n    # Filter genes and cells based on counts (e.g., minimum number of counts per gene and cell)\n    sc.pp.filter_genes(adata, min_counts=1)  # Keep genes with at least 1 count in any cell\n    sc.pp.filter_cells(adata, min_genes=1)   # Keep cells with at least 1 gene expression\n\n    # Normalize the data (total count normalization)\n    sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize each cell to 10,000 counts\n\n    # Log transformation of the normalized data\n    sc.pp.log1p(adata)\n\n    # Highly Variable Genes (optional but recommended for downstream analysis)\n    sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=10, min_disp=0.5)\n    adata = adata[:, adata.var.highly_variable]\n\n    return adata\n\n# Evaluate model performance using various metrics\ndef evaluate_model(true_labels, pred_labels):\n    \"\"\"\n    Evaluate the performance of a model using accuracy, F1 score, ARI, and NMI.\n    \n    Parameters:\n    - true_labels: array, ground truth labels.\n    - pred_labels: array, predicted labels.\n    \n    Returns:\n    - metrics: dict, containing the evaluation metrics.\n    \"\"\"\n    metrics = {}\n    metrics['accuracy'] = accuracy_score(true_labels, pred_labels)\n    metrics['f1_score'] = f1_score(true_labels, pred_labels, average='weighted')\n    metrics['ari'] = adjusted_rand_score(true_labels, pred_labels)\n    metrics['nmi'] = normalized_mutual_info_score(true_labels, pred_labels, average_method='arithmetic')\n    return metrics\n\ndata_dir = \"/kaggle/input/test_data/\"\nmodel_path= \"/kaggle/working/model-0700.pt\"\ntry:\n    covid19_data = load_dataset(\"covid19\", data_dir)\n    print(f\"Loaded COVID-19 dataset with shape {covid19_data.shape}\")\nexcept ValueError as e:\n    print(e)\ntry:\n    healthy_lung_data = load_dataset(\"healthy_lung\", data_dir)\n    print(f\"Loaded Healthy Lung Tissue dataset with shape {healthy_lung_data.shape}\")\nexcept ValueError as e:\n    print(e)\ntry:\n    lung_cancer_data = load_dataset(\"lung_cancer\", data_dir)\n    print(f\"Loaded Lung Cancer dataset with shape {lung_cancer_data.shape}\")\nexcept ValueError as e:\n    print(e)\n\ncovid19_data = preprocess_data(covid19_data)\nhealthy_lung_data = preprocess_data(healthy_lung_data)\nlung_cancer_data = preprocess_data(lung_cancer_data)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = load_model(model_path, device)\ninputs = torch.tensor(covid19_data.X, dtype=torch.float32).to(device)\nwith torch.no_grad():  # No gradient calculation needed during inference\n    pred_labels = model(inputs)\npred_labels = torch.argmax(pred_labels, dim=1).cpu().numpy() \n\nmetrics = evaluate_model(true_labels, pred_labels)\nprint(\"Model evaluation metrics:\", metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}